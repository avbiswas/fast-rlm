{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"fast-rlm","text":"<p>Recursive Language Models</p> <p>fast-rlm is an inference technique where an LLM interacts with arbitrarily long prompts through an external REPL. The LLM can write code to explore, decompose, and transform the prompt. It can recursively invoke sub-agents to complete smaller subtasks. Crucially, sub-agent responses are not automatically loaded into the parent agent's context \u2014 they are returned as symbols or variables inside the parent's REPL.</p> <p> Install  Quick Start</p>"},{"location":"#how-it-works","title":"How it works","text":"<pre><code>User Query\n    |\n    v\nRoot Agent (primary_agent)\n    |-- writes Python code in REPL\n    |-- calls llm_query() to spawn sub-agents\n    |       |\n    |       v\n    |   Sub-Agent (sub_agent)\n    |       |-- explores a chunk of context\n    |       |-- returns result as a variable\n    |       v\n    |   (result flows back as symbol, not raw text)\n    |\n    v\nFinal Answer\n</code></pre> <p>The root agent orchestrates the task by writing Python code. When it needs help with a subtask, it calls <code>llm_query()</code> which spawns a child agent. Child agents can spawn their own children, up to <code>max_depth</code> levels deep. Each agent has a budget of <code>max_calls_per_subagent</code> LLM calls.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Recursive decomposition \u2014 agents spawn sub-agents to handle subtasks</li> <li>REPL-based reasoning \u2014 agents write and execute Python code iteratively</li> <li>Budget controls \u2014 set hard limits on depth, calls, and dollar spend</li> <li>Any OpenAI-compatible API \u2014 works with OpenRouter, OpenAI, local models, etc.</li> <li>Structured logging \u2014 every step logged as JSONL, viewable in an interactive TUI</li> </ul>"},{"location":"#learn-more","title":"Learn More","text":"<p> Read the original RLM paper on arXiv</p> <p>Note</p> <p>fast-rlm is an independent implementation of the ideas from the RLM paper. This project is not affiliated with the original authors.</p>"},{"location":"#support","title":"Support","text":"<p>If you find this helpful, consider supporting on Patreon \u2014 it hosts all code, projects, slides, and write-ups from the YouTube channel.</p> <p> Become a Patron</p>"},{"location":"development/benchmarks/","title":"Benchmarks","text":"<p>fast-rlm includes evaluation scripts under <code>benchmarks/</code> for testing against standard long-context datasets.</p>"},{"location":"development/benchmarks/#setup","title":"Setup","text":"<p>Install benchmark dependencies:</p> <pre><code>uv sync --extra benchmarks\n</code></pre> <p>This adds the <code>datasets</code> library from Hugging Face.</p>"},{"location":"development/benchmarks/#available-benchmarks","title":"Available Benchmarks","text":""},{"location":"development/benchmarks/#longbench-narrativeqa","title":"LongBench (NarrativeQA)","text":"<p>Dataset: THUDM/LongBench \u2014 a multi-task benchmark for long context understanding.</p> <p>What it tests: Reading comprehension over long narratives. The agent receives a full story plus a question, and must find the answer by exploring the text through its REPL.</p> <pre><code>uv run benchmarks/longbench_benchmark.py\n</code></pre> Full source: <code>benchmarks/longbench_benchmark.py</code> <pre><code>import fast_rlm\nfrom datasets import load_dataset\n\nds = load_dataset(\"THUDM/LongBench\",\n                  \"narrativeqa\",\n                  split=\"test\",\n                  trust_remote_code=True)\nidx = 140\n\nexample = ds[idx]\n\nquery = f\"\"\"\n{example['input']}\n\n{example['context']}\n\"\"\"\n\ndata = fast_rlm.run(query, prefix=f\"longbench_hotpot_idx{idx}\")\nprint(\"Expected answer: \", example['answers'])\n</code></pre> <p>To test a different example, change <code>idx</code>:</p> <pre><code>idx = 100  # try different indices\n</code></pre>"},{"location":"development/benchmarks/#oolong-synth","title":"Oolong Synth","text":"<p>Dataset: oolongbench/oolong-synth \u2014 synthetic long-context tasks including timeline ordering, user tracking, and counting.</p> <p>What it tests: Precise information extraction from very long synthetic contexts. Tasks include tracking timelines, counting occurrences, and following user actions across large documents.</p> <pre><code>uv run benchmarks/oolong_synth_benchmark.py\n</code></pre> Full source: <code>benchmarks/oolong_synth_benchmark.py</code> <pre><code>import fast_rlm\nfrom datasets import load_dataset\n\nds = load_dataset(\"oolongbench/oolong-synth\",\n                  split=\"test\")\nidx = 100\n\nexample = ds[idx]\nprint(example['answer'])\n\nquery = f\"\"\"\n{example['context_window_text_with_labels']}\n\n{example['question']}\n\"\"\"\n\ndata = fast_rlm.run(query, prefix=f\"oolong_synth_idx{idx}\")\nprint(\"Expected answer: \", example['answer'])\n</code></pre> <p>You can filter by task type:</p> <pre><code># Available task groups: 'timeline', 'user', 'counting'\nds = ds.filter(lambda x: x['task_group'] == 'counting')\n</code></pre>"},{"location":"development/benchmarks/#adding-new-benchmarks","title":"Adding New Benchmarks","text":"<p>Create a new file in <code>benchmarks/</code>. The pattern is simple:</p> <pre><code>import fast_rlm\nfrom datasets import load_dataset\n\n# 1. Load a dataset\nds = load_dataset(\"your/dataset\", split=\"test\")\n\n# 2. Pick an example\nexample = ds[0]\n\n# 3. Build a query (question + context)\nquery = f\"{example['question']}\\n\\n{example['context']}\"\n\n# 4. Run it\nresult = fast_rlm.run(query, prefix=\"my_benchmark\")\n\n# 5. Compare\nprint(\"Got:\", result[\"results\"])\nprint(\"Expected:\", example[\"answer\"])\nprint(\"Cost:\", result[\"usage\"][\"cost\"])\n</code></pre> <p>The <code>usage</code> field in every result gives you per-run cost and token tracking, useful for comparing efficiency across models and configurations.</p>"},{"location":"development/from-source/","title":"Development from Source","text":""},{"location":"development/from-source/#prerequisites","title":"Prerequisites","text":"<ul> <li>Deno 2+</li> <li>Bun (for the log viewer)</li> <li>uv (for Python/benchmarks)</li> </ul> <p>On Windows, you can install Deno with npm:</p> <pre><code>npm install -g deno\n</code></pre>"},{"location":"development/from-source/#setup","title":"Setup","text":"<pre><code>git clone https://github.com/avbiswas/fast-rlm.git\ncd fast-rlm\n</code></pre>"},{"location":"development/from-source/#install-log-viewer-dependencies","title":"Install log viewer dependencies","text":"<pre><code>cd tui_log_viewer &amp;&amp; bun install &amp;&amp; cd ..\n</code></pre>"},{"location":"development/from-source/#set-your-api-key","title":"Set your API key","text":"<p>Create a <code>.env</code> file in the project root:</p> <pre><code>RLM_MODEL_API_KEY=sk-or-...\nRLM_MODEL_BASE_URL=https://openrouter.ai/api/v1\n</code></pre> <p>Or use <code>.envrc</code> with direnv:</p> <pre><code>export RLM_MODEL_API_KEY=sk-or-...\nexport RLM_MODEL_BASE_URL=https://openrouter.ai/api/v1  # optional, this is the default\n</code></pre> Variable Description Default <code>RLM_MODEL_API_KEY</code> API key for your LLM provider (required) <code>RLM_MODEL_BASE_URL</code> OpenAI-compatible base URL <code>https://openrouter.ai/api/v1</code>"},{"location":"development/from-source/#configuration","title":"Configuration","text":"<p>Edit <code>rlm_config.yaml</code> at the project root:</p> <pre><code>max_calls_per_subagent: 20\nmax_depth: 3\ntruncate_len: 2000\nprimary_agent: \"z-ai/glm-5\"\nsub_agent: \"minimax/minimax-m2.5\"\nmax_money_spent: 1.0\n</code></pre>"},{"location":"development/from-source/#running","title":"Running","text":"<pre><code># Run the counting-r example\ndeno task test_counting_r\n\n# Run the subagent directly\necho \"What is 2+2?\" | deno task subagent\n\n# View logs\n./viewlog logs/&lt;logfile&gt;.jsonl\n</code></pre>"},{"location":"development/from-source/#editable-python-install","title":"Editable Python install","text":"<p>To develop the Python package locally:</p> <pre><code>uv pip install -e .\n</code></pre> <p>Changes to <code>fast_rlm/</code> and <code>src/</code> are reflected immediately \u2014 no rebuild needed.</p>"},{"location":"development/from-source/#project-structure","title":"Project structure","text":"<pre><code>fast-rlm/\n\u251c\u2500\u2500 fast_rlm/              # Python package\n\u2502   \u251c\u2500\u2500 __init__.py        # Public API: run(), RLMConfig\n\u2502   \u251c\u2500\u2500 _runner.py         # Engine discovery, config merge, subprocess\n\u2502   \u2514\u2500\u2500 _cli.py            # fast-rlm-log CLI entry point\n\u251c\u2500\u2500 src/                   # TypeScript engine (Deno)\n\u2502   \u251c\u2500\u2500 subagents.ts       # Core recursive agent loop\n\u2502   \u251c\u2500\u2500 call_llm.ts        # LLM API client\n\u2502   \u251c\u2500\u2500 prompt.ts          # System prompt\n\u2502   \u251c\u2500\u2500 logging.ts         # Pino-based JSONL logger\n\u2502   \u251c\u2500\u2500 ui.ts              # Terminal UI (spinners, boxes)\n\u2502   \u2514\u2500\u2500 usage.ts           # Token/cost tracking\n\u251c\u2500\u2500 tui_log_viewer/        # OpenTUI log viewer (Bun)\n\u251c\u2500\u2500 benchmarks/            # Evaluation scripts\n\u251c\u2500\u2500 deno.json              # Deno config + task definitions\n\u251c\u2500\u2500 rlm_config.yaml        # Default agent configuration\n\u2514\u2500\u2500 pyproject.toml         # Python build config (hatchling)\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#1-install-fast-rlm","title":"1. Install fast-rlm","text":"<pre><code>pip install fast-rlm\n</code></pre>"},{"location":"getting-started/installation/#2-install-deno","title":"2. Install Deno","text":"<p>fast-rlm requires Deno 2+ as its runtime engine.</p> macOS / LinuxWindows <pre><code>curl -fsSL https://deno.land/install.sh | sh\n</code></pre> <p>Then add Deno to your <code>PATH</code>:</p> <pre><code>export DENO_INSTALL=\"$HOME/.deno\"\nexport PATH=\"$DENO_INSTALL/bin:$PATH\"\n</code></pre> <pre><code>irm https://deno.land/install.ps1 | iex\n</code></pre> <p>Or install via npm:</p> <pre><code>npm install -g deno\n</code></pre> <p>Verify the installation:</p> <pre><code>deno --version\n</code></pre>"},{"location":"getting-started/installation/#3-set-your-api-key","title":"3. Set your API key","text":"<p>fast-rlm uses OpenRouter by default. Set your API key:</p> <pre><code>export RLM_MODEL_API_KEY=sk-or-...\n</code></pre> <p>Tip</p> <p>Add this to your <code>.bashrc</code>, <code>.zshrc</code>, or <code>.envrc</code> so it persists across sessions.</p>"},{"location":"getting-started/installation/#4-optional-install-bun","title":"4. (Optional) Install Bun","text":"<p>Only needed if you want the interactive TUI log viewer (<code>fast-rlm-log &lt;file&gt; --tui</code>).</p> macOS / LinuxWindows <pre><code>curl -fsSL https://bun.sh/install | bash\n</code></pre> <p>Then add Bun to your <code>PATH</code> (the installer will print the exact lines, but typically):</p> <pre><code>export BUN_INSTALL=\"$HOME/.bun\"\nexport PATH=\"$BUN_INSTALL/bin:$PATH\"\n</code></pre> <pre><code>powershell -c \"irm bun.sh/install.ps1 | iex\"\n</code></pre> <p>Or install via npm:</p> <pre><code>npm install -g bun\n</code></pre> <p>Verify the installation:</p> <pre><code>bun --version\n</code></pre> <p>Tip</p> <p>Add the <code>PATH</code> export to your <code>.bashrc</code>, <code>.zshrc</code>, or <code>.envrc</code> so <code>bun</code> is available in every new shell session.</p>"},{"location":"getting-started/installation/#environment-variables","title":"Environment Variables","text":"Variable Description Default <code>RLM_MODEL_API_KEY</code> API key for your LLM provider (required) <code>RLM_MODEL_BASE_URL</code> OpenAI-compatible base URL <code>https://openrouter.ai/api/v1</code> <p>You can point fast-rlm at any OpenAI-compatible API:</p> <pre><code>export RLM_MODEL_API_KEY=sk-...\nexport RLM_MODEL_BASE_URL=https://api.openai.com/v1\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":""},{"location":"getting-started/quickstart/#basic-usage","title":"Basic usage","text":"<pre><code>import fast_rlm\n\nresult = fast_rlm.run(\"Generate 50 fruits and count number of r\")\nprint(result[\"results\"])\nprint(result[\"usage\"])\n</code></pre> <p>The returned dict contains:</p> <pre><code>{\n    \"results\": ...,        # the agent's final answer\n    \"log_file\": \"...\",     # path to the JSONL log\n    \"usage\": {\n        \"prompt_tokens\": 12345,\n        \"completion_tokens\": 678,\n        \"total_tokens\": 13023,\n        \"cached_tokens\": 5000,\n        \"reasoning_tokens\": 200,\n        \"cost\": 0.0342\n    }\n}\n</code></pre>"},{"location":"getting-started/quickstart/#arbitrarily-long-context","title":"Arbitrarily long context","text":"<p>The key idea behind RLMs is that the prompt can be arbitrarily long \u2014 far beyond any model's context window. The agent explores it programmatically through the REPL rather than trying to fit it all into a single call.</p> <pre><code>import fast_rlm\n\ntranscripts = open(\"lex_fridman_all_transcripts.txt\").read()  # millions of tokens\n\nresult = fast_rlm.run(\n    \"Here are the transcripts of all Lex Fridman podcasts. \"\n    \"Summarize what the first 5 Machine Learning guests had to say about AGI.\\n\\n\"\n    + transcripts\n)\nprint(result[\"results\"])\n</code></pre> <p>The agent will write code to search, filter, and chunk the transcripts on its own \u2014 no manual splitting required.</p>"},{"location":"getting-started/quickstart/#with-configuration","title":"With configuration","text":"<pre><code>from fast_rlm import run, RLMConfig\n\nconfig = RLMConfig.default()\nconfig.primary_agent = \"minimax/minimax-m2.5\"\nconfig.sub_agent = \"minimax/minimax-m2.5\"\nconfig.max_depth = 5\nconfig.max_money_spent = 2.0\n\nresult = run(\n    \"Count the r's in 50 fruit names\",\n    prefix=\"r_count\",\n    config=config,\n)\n</code></pre>"},{"location":"getting-started/quickstart/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>query</code> <code>str</code> (required) The question or context to process <code>prefix</code> <code>str</code> <code>None</code> Log filename prefix (e.g. <code>\"r_count\"</code> \u2192 <code>r_count_2026-02-23T...</code>) <code>config</code> <code>RLMConfig</code> or <code>dict</code> <code>None</code> Config overrides (see Configuration) <code>verbose</code> <code>bool</code> <code>True</code> Stream engine output to terminal"},{"location":"getting-started/quickstart/#quiet-mode","title":"Quiet mode","text":"<p>To suppress all terminal output and just get the result:</p> <pre><code>result = fast_rlm.run(\"What is 2+2?\", verbose=False)\n</code></pre>"},{"location":"guide/configuration/","title":"Configuration","text":""},{"location":"guide/configuration/#rlmconfig","title":"RLMConfig","text":"<p>All configuration is managed through the <code>RLMConfig</code> dataclass. Every field has a sensible default.</p> <pre><code>from fast_rlm import RLMConfig\n\nconfig = RLMConfig.default()\n</code></pre>"},{"location":"guide/configuration/#fields","title":"Fields","text":"Field Type Default Description <code>primary_agent</code> <code>str</code> <code>z-ai/glm-5</code> Model used for the root agent <code>sub_agent</code> <code>str</code> <code>minimax/minimax-m2.5</code> Model used for child subagents <code>max_depth</code> <code>int</code> <code>3</code> Max recursive subagent depth <code>max_calls_per_subagent</code> <code>int</code> <code>20</code> Max LLM calls a single subagent can make <code>truncate_len</code> <code>int</code> <code>2000</code> Output characters shown to the LLM per step <code>max_money_spent</code> <code>float</code> <code>1.0</code> Hard budget cap in USD \u2014 raises an error if exceeded <code>max_completion_tokens</code> <code>int</code> <code>50000</code> Max total completion tokens across all subagents \u2014 raises an error if exceeded <code>max_prompt_tokens</code> <code>int</code> <code>200000</code> Max total prompt tokens across all subagents \u2014 raises an error if exceeded"},{"location":"guide/configuration/#modifying-config","title":"Modifying config","text":"<p><code>RLMConfig</code> is a dataclass \u2014 just set attributes directly:</p> <pre><code>config = RLMConfig.default()\nconfig.primary_agent = \"openai/gpt-5.2\"\nconfig.max_depth = 5\nconfig.max_money_spent = 3.0\nconfig.max_completion_tokens = 100000\nconfig.max_prompt_tokens = 500000\n</code></pre>"},{"location":"guide/configuration/#using-a-dict","title":"Using a dict","text":"<p>You can also pass a plain dict if you prefer:</p> <pre><code>from fast_rlm import run\n\nresult = run(\n    \"Summarize this document\",\n    config={\"primary_agent\": \"openai/gpt-5.2-codex\", \"max_depth\": 5},\n)\n</code></pre> <p>Dict values are merged on top of the defaults from <code>rlm_config.yaml</code>.</p> <p>Cost tracking depends on your provider</p> <p>Not all API providers return cost information in their responses. OpenRouter includes cost data, but OpenAI and most other providers do not. If your provider doesn't return cost, <code>max_money_spent</code> won't be able to enforce a budget and cost will show as \"Unknown\" in the UI. In that case, use <code>max_completion_tokens</code> and <code>max_prompt_tokens</code> to control spending instead \u2014 these work with any provider since token counts are always returned.</p>"},{"location":"guide/configuration/#how-config-merging-works","title":"How config merging works","text":"<ol> <li>Defaults are loaded from the bundled <code>rlm_config.yaml</code></li> <li>Your overrides (from <code>RLMConfig</code> or <code>dict</code>) are applied on top</li> <li>The merged config is written to a temp file and passed to the engine</li> </ol> <p>This means you only need to specify what you want to change \u2014 everything else keeps its default value.</p>"},{"location":"guide/configuration/#model-names","title":"Model names","text":"<p>fast-rlm uses OpenRouter by default, so model names follow the OpenRouter format: <code>provider/model-name</code>.</p> <p>Since agents write and execute Python code in a REPL, choose models that are strong at coding. Recommended models:</p> Model Name GPT-5.2 Codex <code>openai/gpt-5.2-codex</code> Claude Sonnet 4.6 <code>anthropic/claude-sonnet-4-6</code> Gemini 3.1 Pro Preview <code>google/gemini-3.1-pro-preview</code> MiniMax M2.5 <code>minimax/minimax-m2.5</code> GLM-5 <code>z-ai/glm-5</code> <p>Browse the full list at openrouter.ai/models.</p> <p>Using a different provider</p> <p>If you're pointing <code>RLM_MODEL_BASE_URL</code> at a different API (e.g. OpenAI directly), use that provider's model names instead (e.g. <code>gpt-5.2-codex</code> instead of <code>openai/gpt-5.2-codex</code>).</p>"},{"location":"guide/log-viewer/","title":"Log Viewer","text":"<p>Every run saves a <code>.jsonl</code> log file to <code>logs/</code>. The log captures every step: code generated, output produced, usage stats, and final results across all agents and sub-agents.</p> <p></p>"},{"location":"guide/log-viewer/#stats-default","title":"Stats (default)","text":"<p>Print a summary of any log file \u2014 no extra dependencies required:</p> <pre><code>fast-rlm-log logs/run_2026-02-23T20-33-30-936Z.jsonl\n</code></pre> <p>Output:</p> <pre><code>Log entries:  42\nTotal runs:   5\nRoot runs:    1\nMax depth:    2\nTotal tokens: 45,230\nTotal cost:   $0.034521\n</code></pre>"},{"location":"guide/log-viewer/#interactive-tui","title":"Interactive TUI","text":"<p>For a full interactive viewer with code highlighting, step navigation, and reasoning traces:</p> <pre><code>fast-rlm-log logs/run_2026-02-23T20-33-30-936Z.jsonl --tui\n</code></pre> <p>Requires Bun</p> <p>The TUI viewer is built with OpenTUI and requires Bun. Dependencies are installed automatically on first run.</p>"},{"location":"guide/log-viewer/#tui-controls","title":"TUI controls","text":"Key Action <code>Up</code> / <code>Down</code> Navigate steps in current run <code>Left</code> / <code>Right</code> Go to parent / child subagent <code>Tab</code> / <code>Shift+Tab</code> Next / previous sibling subagent <code>H</code> / <code>J</code> Scroll code panel up / down <code>K</code> / <code>L</code> Scroll output panel up / down <code>R</code> Toggle reasoning trace modal <code>O</code> Toggle final output modal <code>q</code> / <code>Ctrl+C</code> Quit"},{"location":"guide/log-viewer/#programmatic-access","title":"Programmatic access","text":"<p>The log file path is included in every <code>run()</code> result:</p> <pre><code>import fast_rlm\n\nresult = fast_rlm.run(\"What is 2+2?\")\nprint(result[\"log_file\"])  # e.g. \"./logs/run_2026-02-23T20-33-30-936Z.jsonl\"\n</code></pre> <p>The log is standard JSONL \u2014 each line is a JSON object you can parse with any tool.</p>"},{"location":"guide/tips/","title":"Best Practices &amp; Troubleshooting","text":""},{"location":"guide/tips/#best-practices","title":"Best practices","text":""},{"location":"guide/tips/#place-your-task-at-the-start-or-end-of-the-prompt","title":"Place your task at the start or end of the prompt","text":"<p>RLMs explore long prompts by writing code, slicing context, and finding keywords \u2014 the REPL restricts the amount of context the LLM can see (you can configure this using the config param <code>truncate_len</code>). If your task description is buried in the middle, the agent may struggle to find it. Always put the task at the very top or bottom of the prompt so the agent sees it immediately.</p> <pre><code># Good \u2014 task is at the top, data follows\nresult = fast_rlm.run(\n    \"Summarize the key takeaways from these meeting notes.\\n\\n\"\n    + meeting_notes\n)\n\n# Also good \u2014 data first, task at the end\nresult = fast_rlm.run(\n    meeting_notes + \"\\n\\n\"\n    \"Given the meeting notes above, summarize the key takeaways.\"\n)\n</code></pre>"},{"location":"guide/tips/#mark-structured-data-with-backtick-blocks","title":"Mark structured data with backtick blocks","text":"<p>If your data has a specific structure (JSON, CSV, XML, etc.), wrap it in fenced code blocks and tell the agent what format it is. This makes it much easier for the agent to extract and parse the data programmatically.</p> <pre><code>result = fast_rlm.run(\n    \"The following is a CSV of sales records. \"\n    \"Find the top 5 products by revenue.\\n\\n\"\n    \"```csv\\n\"\n    + csv_data +\n    \"\\n```\"\n)\n</code></pre>"},{"location":"guide/tips/#use-strong-coding-models","title":"Use strong coding models","text":"<p>RLM agents write and execute Python code to solve tasks \u2014 model quality matters a lot. Check coding benchmarks and pick models that perform well on code generation. See the Configuration page for recommended models.</p>"},{"location":"guide/tips/#add-domain-context-when-needed","title":"Add domain context when needed","text":"<p>If the domain is obscure or specialized, you can feed additional documentation into the prompt. Just make sure you tell the agent how the data is organized so it can navigate efficiently.</p> <pre><code>docs = open(\"internal_api_reference.md\").read()\nlogs = open(\"error_logs.txt\").read()\n\nresult = fast_rlm.run(\n    \"I have provided two documents below, separated by markdown ## headers.\\n\"\n    \"## API Reference\\n\" + docs + \"\\n\\n\"\n    \"## Error Logs\\n\" + logs + \"\\n\\n\"\n    \"Using the API reference, diagnose why the errors in the logs are occurring.\"\n)\n</code></pre>"},{"location":"guide/tips/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/tips/#check-your-logs","title":"Check your logs","text":"<p>Every run produces a <code>.jsonl</code> log file. Use the Log Viewer to inspect what the agent is doing step by step \u2014 what code it wrote, what output it saw, and where it went wrong.</p> <pre><code>fast-rlm-log logs/run_xxx.jsonl --tui\n</code></pre>"},{"location":"guide/tips/#start-with-strict-limits","title":"Start with strict limits","text":"<p>In early experiments, keep <code>max_depth</code>, <code>max_calls_per_subagent</code>, and <code>max_money_spent</code> low. This lets you verify the agent is on the right track before scaling up. If the agent consistently fails or goes off course, it usually means the task prompt needs adjustment rather than more budget.</p> <pre><code>from fast_rlm import run, RLMConfig\n\nconfig = RLMConfig.default()\nconfig.max_depth = 2\nconfig.max_calls_per_subagent = 10\nconfig.max_money_spent = 0.50\n\nresult = run(\"Your task here...\", config=config)\n</code></pre>"},{"location":"guide/tips/#things-to-do-before-increasing-budget","title":"Things to do before increasing budget","text":"<p>If results are poor, work through this list before raising <code>max_depth</code>, <code>max_calls_per_subagent</code>, or <code>max_money_spent</code>:</p> <ol> <li>Review the logs \u2014 Use the Log Viewer to see exactly where the agent went wrong. This tells you what to fix.</li> <li>Make the task description clearer \u2014 Be specific about what you want. Vague prompts lead to wasted calls.</li> <li>Move the task to the top or bottom of the prompt \u2014 Don't let it get lost in the middle of a large context.</li> <li>Try a stronger coding model \u2014 Switch <code>primary_agent</code> or <code>sub_agent</code> to a model with better coding benchmarks. A smarter model often solves in fewer calls what a weaker model can't solve at all.</li> <li>Mark data formats explicitly \u2014 Wrap JSON, CSV, or structured data in fenced code blocks and name the format in the prompt.</li> <li>Inject additional reference docs \u2014 If the domain is specialized, add relevant documentation to the context so the agent doesn't have to guess.</li> <li>Increase <code>truncate_len</code> \u2014 If the agent is missing important output because it gets clipped, bump this value so it can see more per step.</li> </ol>"}]}